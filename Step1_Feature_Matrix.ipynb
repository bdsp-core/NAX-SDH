{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import all necessary modules. \n",
    "#Optional steps 1-5 can be used to make new sampling cohorts, however, you must annually annotate the results from steps 3 and 4 by putting the file paths in the annotation tool\n",
    "#and checking if SDH is present in the patient notes using key words. \n",
    "#Steps 1-25 can be used to replicate this data. Just click play and run them in order. This folder should have all the files needed to replicate this data. \n",
    "#If part of the Prophet project, please skip steps 6, 13, 14, 18, 19, 20, and 21 for the data. \n",
    "\n",
    "#Import necessary modules\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load in Positive and Negative Sampling Cohorts for MGB and BIDMC\n",
    "MGB_plus=pd.read_csv(\"/home/gregory178/Desktop/NAX project/NAX_SDH/MGB_sampling_cohort_ICD+_discharge_notes.csv\")\n",
    "MGB_minus=pd.read_csv(\"/home/gregory178/Desktop/NAX project/NAX_SDH/MGB_sampling_cohort_ICD_minus_discharge_notes.csv\")\n",
    "BI_plus=pd.read_csv(\"/home/gregory178/Desktop/NAX project/NAX_SDH/BI_sampling_cohort_ICD+_initial_notes.csv\")\n",
    "BI_minus=pd.read_csv(\"/home/gregory178/Desktop/NAX project/NAX_SDH/BI_sampling_cohort_ICD_minus_initial_notes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concatenate the BIDMC+/- and MGB+/- notes together. \n",
    "#Make MGB and BIDMC seperate for CPT codes because MGB and BIDMC data work differently for it. \n",
    "MGB_Complete_Notes = pd.concat([MGB_plus,MGB_minus], ignore_index=True)\n",
    "BIDMC_Complete_Notes= pd.concat([BI_plus,BI_minus], ignore_index=True)\n",
    "\n",
    "MGB_Complete_Notes.to_csv(\"MGB_Complete_Notes.csv\", index=False)\n",
    "BIDMC_Complete_Notes.to_csv(\"BIDMC_Complete_Notes.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500\n",
      "Index(['BDSPPatientID', 'NoteContent', 'ContactDate', 'WordCount',\n",
      "       'NoteFileName', 'Site'],\n",
      "      dtype='object')\n",
      "      BDSPPatientID                                        NoteContent  \\\n",
      "0         120109726    Physician ***** *****       Admit date: ****...   \n",
      "1         111971091    Physician ***** *****       Admit date: ****...   \n",
      "2         114651683    Physician ***** *****       Admit date: ****...   \n",
      "3         115288640    Physician ***** *****       Admit date: ****...   \n",
      "4         115389340    Physician ***** *****       Admit date: ****...   \n",
      "...             ...                                                ...   \n",
      "1495      116101588    Physician ***** *****       Admit date: ****...   \n",
      "1496      111193161   Note *****: Detach Note + Clinical Encounter ...   \n",
      "1497      117924962    Physician ***** *****       Admit date: ****...   \n",
      "1498      121552284    Physician ***** *****       Admit date: ****...   \n",
      "1499      122349255    Physician ***** *****       Admit date: ****...   \n",
      "\n",
      "     ContactDate  WordCount                                NoteFileName Site  \n",
      "0     2019-12-01       2126   Notes_13393227243_2508134861_20191201.txt  MGB  \n",
      "1     2021-05-30       3141   Notes_13517098931_6064385669_20210530.txt  MGB  \n",
      "2     2022-03-09       3889   Notes_13621620103_9363901699_20220309.txt  MGB  \n",
      "3     2021-03-02       2513   Notes_13554067774_6841967671_20210302.txt  MGB  \n",
      "4     2019-12-02       1944   Notes_13481394171_4791230696_20191202.txt  MGB  \n",
      "...          ...        ...                                         ...  ...  \n",
      "1495  2017-09-03       1453   Notes_13359486638_1627164244_20170903.txt  MGB  \n",
      "1496  2022-11-01       1226  Notes_13746232215_10368338628_20221101.txt  MGB  \n",
      "1497  2021-03-30       2284   Notes_13471151986_4730414003_20210330.txt  MGB  \n",
      "1498  2017-07-30       2440   Notes_13358171145_2046736196_20170730.txt  MGB  \n",
      "1499  2017-01-10       2282   Notes_13328448893_1687300589_20170110.txt  MGB  \n",
      "\n",
      "[1500 rows x 6 columns]\n",
      "1500\n",
      "Index(['BDSPPatientID', 'NoteContent', 'ContactDate', 'WordCount',\n",
      "       'NoteFileName', 'Site'],\n",
      "      dtype='object')\n",
      "      BDSPPatientID                                        NoteContent  \\\n",
      "0         150683141  \\n\\nNote Date: *****/*****/*****\\n\\nNote Type:...   \n",
      "1         150015356  \\n\\nNote Date: *****/*****/*****\\n\\nNote Type:...   \n",
      "2         150000686  \\n\\nNote Date: *****/*****/*****\\n\\nNote Type:...   \n",
      "3         150000451  \\n\\nNote Date: *****/*****/*****\\n\\nNote Type:...   \n",
      "4         150014747  \\n\\nNote Date: *****/*****/*****\\n\\nNote Type:...   \n",
      "...             ...                                                ...   \n",
      "1495      150057113  \\n\\nNote Date: *****/*****/*****\\n\\nNote Type:...   \n",
      "1496      150740592  \\n\\nNote Date: *****/*****/*****\\n\\nNote Type:...   \n",
      "1497      150616738  \\n\\nNote Date: *****/*****/*****\\n\\nNote Type:...   \n",
      "1498      150044220  \\n\\nNote Date: *****/*****/*****\\n\\nNote Type:...   \n",
      "1499      150028704  \\n\\nNote Date: *****/*****/*****\\n\\nNote Type:...   \n",
      "\n",
      "     ContactDate  WordCount                               NoteFileName   Site  \n",
      "0     2016-06-01       1227  Notes_1130542094_11409543435_20160601.txt  BIDMC  \n",
      "1     2017-04-07       1663   Notes_1129874480_3082345375_20170407.txt  BIDMC  \n",
      "2     2020-01-11        912    Notes_1129859284_277121157_20200111.txt  BIDMC  \n",
      "3     2016-01-22       1683    Notes_1129859570_238148159_20160122.txt  BIDMC  \n",
      "4     2016-06-25       1276   Notes_1129873727_3024909815_20160625.txt  BIDMC  \n",
      "...          ...        ...                                        ...    ...  \n",
      "1495  2018-09-21       1472   Notes_1129915942_2473056145_20180921.txt  BIDMC  \n",
      "1496  2010-10-10       1172   Notes_1130599114_1436450624_20101010.txt  BIDMC  \n",
      "1497  2019-08-06       1187   Notes_1130475398_1086092807_20190806.txt  BIDMC  \n",
      "1498  2016-12-04        738   Notes_1129902668_2977113246_20161204.txt  BIDMC  \n",
      "1499  2020-11-04       1187  Notes_1129887208_12495579047_20201104.txt  BIDMC  \n",
      "\n",
      "[1500 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "#Verifying it concatenated correctly.\n",
    "# MGB\n",
    "df=pd.read_csv('/home/gregory178/Desktop/NAX project/NAX_SDH/MGB_Complete_Notes.csv')\n",
    "print(len(df))\n",
    "print(df.columns)\n",
    "print(df)\n",
    "\n",
    "# BIDMC\n",
    "df1=pd.read_csv('/home/gregory178/Desktop/NAX project/NAX_SDH/BIDMC_Complete_Notes.csv')\n",
    "print(len(df1))\n",
    "print(df1.columns)\n",
    "print(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BDSPPatientID</th>\n",
       "      <th>NoteContent</th>\n",
       "      <th>ContactDate</th>\n",
       "      <th>WordCount</th>\n",
       "      <th>NoteFileName</th>\n",
       "      <th>Site</th>\n",
       "      <th>MRI</th>\n",
       "      <th>CT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>150683141</td>\n",
       "      <td>\\n\\nNote Date: *****/*****/*****\\n\\nNote Type:...</td>\n",
       "      <td>2016-06-01</td>\n",
       "      <td>1227</td>\n",
       "      <td>Notes_1130542094_11409543435_20160601.txt</td>\n",
       "      <td>BIDMC</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>150015356</td>\n",
       "      <td>\\n\\nNote Date: *****/*****/*****\\n\\nNote Type:...</td>\n",
       "      <td>2017-04-07</td>\n",
       "      <td>1663</td>\n",
       "      <td>Notes_1129874480_3082345375_20170407.txt</td>\n",
       "      <td>BIDMC</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>150000686</td>\n",
       "      <td>\\n\\nNote Date: *****/*****/*****\\n\\nNote Type:...</td>\n",
       "      <td>2020-01-11</td>\n",
       "      <td>912</td>\n",
       "      <td>Notes_1129859284_277121157_20200111.txt</td>\n",
       "      <td>BIDMC</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>150000451</td>\n",
       "      <td>\\n\\nNote Date: *****/*****/*****\\n\\nNote Type:...</td>\n",
       "      <td>2016-01-22</td>\n",
       "      <td>1683</td>\n",
       "      <td>Notes_1129859570_238148159_20160122.txt</td>\n",
       "      <td>BIDMC</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>150014747</td>\n",
       "      <td>\\n\\nNote Date: *****/*****/*****\\n\\nNote Type:...</td>\n",
       "      <td>2016-06-25</td>\n",
       "      <td>1276</td>\n",
       "      <td>Notes_1129873727_3024909815_20160625.txt</td>\n",
       "      <td>BIDMC</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   BDSPPatientID                                        NoteContent  \\\n",
       "0      150683141  \\n\\nNote Date: *****/*****/*****\\n\\nNote Type:...   \n",
       "1      150015356  \\n\\nNote Date: *****/*****/*****\\n\\nNote Type:...   \n",
       "2      150000686  \\n\\nNote Date: *****/*****/*****\\n\\nNote Type:...   \n",
       "3      150000451  \\n\\nNote Date: *****/*****/*****\\n\\nNote Type:...   \n",
       "4      150014747  \\n\\nNote Date: *****/*****/*****\\n\\nNote Type:...   \n",
       "\n",
       "  ContactDate  WordCount                               NoteFileName   Site  \\\n",
       "0  2016-06-01       1227  Notes_1130542094_11409543435_20160601.txt  BIDMC   \n",
       "1  2017-04-07       1663   Notes_1129874480_3082345375_20170407.txt  BIDMC   \n",
       "2  2020-01-11        912    Notes_1129859284_277121157_20200111.txt  BIDMC   \n",
       "3  2016-01-22       1683    Notes_1129859570_238148159_20160122.txt  BIDMC   \n",
       "4  2016-06-25       1276   Notes_1129873727_3024909815_20160625.txt  BIDMC   \n",
       "\n",
       "   MRI  CT  \n",
       "0    0   0  \n",
       "1    0   1  \n",
       "2    0   0  \n",
       "3    0   1  \n",
       "4    0   1  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Adding CPT Columns to the feature matrix.\n",
    "#We will start with MGB CPT Codes. Because there was a CSV for CPT MGB, we will use it to find specific CPT codes for Brain MRI and Head CT.\n",
    "#Read in the CPT data from MGB. \n",
    "CPTs = pd.read_csv('/home/gregory178/Desktop/NAX project/NAX_SDH/patientIDs_CPT_HeadMRICT_MGB.csv')\n",
    "\n",
    "#Filter CPT Patient Ids to only those in your matrix.\n",
    "CPTs = CPTs[CPTs['BDSPPatientID'].isin(set(MGB_Complete_Notes['BDSPPatientID']))]\n",
    "\n",
    "#Convert 'StartDTS' to datetime.\n",
    "CPTs['CPT Date'] = pd.to_datetime(CPTs['StartDTS'], errors='coerce')\n",
    "\n",
    "#Select relevant columns from CPT data.\n",
    "CPTs = CPTs[['BDSPPatientID', 'CPT Date', 'CPT']]\n",
    "\n",
    "#Define function that checks if cpt date and contact date are within 6 months of each other.\n",
    "def within_six_months(contact_date, cpt_date):\n",
    "    return (contact_date >= cpt_date - pd.DateOffset(months=6)) & (contact_date <= cpt_date + pd.DateOffset(months=6))\n",
    "\n",
    "\n",
    "#Initialize columns in matrix for MRI and CT presence. These columns will both be features in the feature matrix.\n",
    "MGB_Complete_Notes['MRI'] = 0\n",
    "MGB_Complete_Notes['CT'] = 0\n",
    "\n",
    "#Convert ContactDate to datetime.\n",
    "MGB_Complete_Notes['ContactDate'] = pd.to_datetime(MGB_Complete_Notes['ContactDate'], errors='coerce')\n",
    "\n",
    "#Iterate over each patient in matrix.\n",
    "for index, row in MGB_Complete_Notes.iterrows():\n",
    "    patient_id = row['BDSPPatientID']\n",
    "    contact_date = row['ContactDate']\n",
    "\n",
    "    #Check for MRI CPTs\n",
    "    for _, cpt_row in CPTs[(CPTs['BDSPPatientID'] == patient_id) & (CPTs['CPT'].isin([70450, 70460, 70470]))].iterrows():\n",
    "        cpt_date = cpt_row['CPT Date']\n",
    "        if pd.notnull(contact_date) and pd.notnull(cpt_date) and within_six_months(contact_date, cpt_date):\n",
    "            MGB_Complete_Notes.at[index, 'MRI'] = 1\n",
    "            #Once found, no need to check further.\n",
    "            break \n",
    "\n",
    "    #Check for CT CPTs\n",
    "    for _, cpt_row in CPTs[(CPTs['BDSPPatientID'] == patient_id) & (CPTs['CPT'].isin([70551, 70552, 70553]))].iterrows():\n",
    "        cpt_date = cpt_row['CPT Date']\n",
    "        if pd.notnull(contact_date) and pd.notnull(cpt_date) and within_six_months(contact_date, cpt_date):\n",
    "            MGB_Complete_Notes.at[index, 'CT'] = 1\n",
    "            #Once found, no need to check further. \n",
    "            break  \n",
    "\n",
    "#Convert resulting MGB_Complete_Notes with updated CT and MRI columns to CSV\n",
    "MGB_Complete_Notes.to_csv('MGB_CPT_.csv', index=False)\n",
    "\n",
    "#We will now make BIDMC CPT columns. \n",
    "# For each patient use regex to search for cpt within note text of matrix to fill in matrix value instead of using cpt codes due to lack of available info on BIDMC.\n",
    "# This code will allow us to assign a 1 to the patient if it contains the regex word or a 0 if it does not. \n",
    "BIDMC_Complete_Notes['MRI'] = BIDMC_Complete_Notes['NoteContent'].str.contains(r'(?:brain mri|mri brain|head mri|mri head)', regex=True, case=False, na=False).astype(int)\n",
    "BIDMC_Complete_Notes['CT'] = BIDMC_Complete_Notes['NoteContent'].str.contains(r'(?:ct head|head ct|brain ct|ct brain|hct)', regex=True, case=False, na=False).astype(int)\n",
    "BIDMC_Complete_Notes.to_csv('BIDMC_CPT_.csv', index=False)\n",
    "\n",
    "#Assigning df and df1 variables to MGB and BIDMC\n",
    "df = pd.read_csv(\"/home/gregory178/Desktop/NAX project/NAX_SDH/MGB_CPT_.csv\")\n",
    "df1 = BIDMC_Complete_Notes\n",
    "\n",
    "#Making sure that the CPT code created the MRI and CT columns.\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/gregory178/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "##Here we are creating the feature matrix using key words for MGB\n",
    "\n",
    "#Importing necessary modules\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "#Initialize the Snowball Stemmer for English\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "#Define keywords and phrases. Put in your own phrases as needed as long as they are stemmed. \n",
    "#Keywords before stemming\n",
    "#  \"acute sdh\", \"brain injury\", \"brain mri\",\n",
    "    # \"burr hole\", \"chronic sdh\", \"craniectomy\", \"craniotomy\", \"ct head\", \"drainage\", \"epidural hemorrhage\",\n",
    "    # \"evacuation\", \"head ct\", \"hematoma\",\n",
    "    # \"herniation\", \"imaging\", \"intracranial hemorrhage\", \"intraparenchymal hemorrhage\",\n",
    "    # \"midline shift\", \"mri brain\", \"mva\", \"mvc\", \"neurosurgery\", \"neurosurgical intervention\",\n",
    "    # \"scan\", \"sdh\", \"subdural\", \"tbi\", \"tentorium\", \"thickness\", \"trauma\"\n",
    "\n",
    "#keywords below after stemming\n",
    "keywords = [\n",
    "    \"acut sdh\", \"brain injuri\", \"brain mri\",\n",
    "    \"burr hole\", \"stabl sdh\", \"craniectomi\", \"craniotomi\", \"ct head\", \"drainag\",\n",
    "    \"evacu\", \"head ct\", \"hematoma\",\"chronic sdh\",\n",
    "    \"herniat\", \"intracrani hemorrhag\", \"intraparenchym hemorrhag\",\n",
    "    \"midlin shift\", \"mva\", \"mvc\", \"neurosurgeri\", \"neurosurg intervent\",\n",
    "    \"scan\", \"sdh\", \"subdur\", \"tbi\", \"tentorium\", \"thick\", \"trauma\", \"prior sdh\", \"recent sdh\", \"resolv sdh\",\n",
    "    \"known sdh\"\n",
    "]\n",
    " \n",
    "# Function to preprocess text (lowercase, stemming using NLTK)\n",
    "def preprocess_text(text):\n",
    "    #Convert text to lowercase\n",
    "    text = text.lower()  \n",
    "    # Replace specific terms as needed to SDH.\n",
    "    text = re.sub(r\"\\bsubdural (hematoma|hemorrhage)\\b\", \"sdh\", text)   \n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "    #Getting rid of extra spacing. \n",
    "    return ' '.join(stemmed_tokens) \n",
    "\n",
    "\n",
    "# Preprocess the 'NoteContent' column to make usre all text is lowered and has one space in between.\n",
    "df['NoteContent'] = df['NoteContent'].apply(preprocess_text)\n",
    "\n",
    "#history pattern. Only want to apply to sdh. \n",
    "history_patterns = {}\n",
    "sdh_pattern = r\"\\b(?:histori\\b|histori:\\b|hx\\b|pth\\b|pmh\\b)[^,.]{0,20}\\bsd(?:h)?\\b\"\n",
    "history_patterns[\"sdh\"] = sdh_pattern\n",
    "\n",
    "\n",
    "# generate neg and pos patterns\n",
    "pos_patterns = {}\n",
    "for phrase in keywords:\n",
    "    #stemmed_phrase = stem_phrase(phrase, stemmer)\n",
    "    words = phrase.split()\n",
    "    pos_patterns[phrase] = r\"\\b\" + r\"\".join([f\"{word}\\s*\" for word in words]) +r\"\\b\"\n",
    "\n",
    "# Adding negator words\n",
    "neg_patterns = {}\n",
    "for phrase in keywords:\n",
    "    #stemmed_phrase = stem_phrase(phrase, stemmer)  # no need since the keywords are already in basic form\n",
    "    words = phrase.split()\n",
    "    neg_patterns[phrase] = r\"\\b\" + r\"(?:(?:negat|no\\b|not\\b)[^,.]{{0,20}}{pattern})|(?:{pattern}[^.]{{0,10}}(?:absent\\b|negat))\".format(\n",
    "        pattern= r''.join([f\"{word}\\s*\" for word in words]) +r\"\\b\" )\n",
    "\n",
    "\n",
    "def check_patterns(text, neg_patterns, pos_patterns, history_patterns):\n",
    "    matches = {f\"{key}_pos\": 0 for key in pos_patterns}\n",
    "    matches.update({f\"{key}_neg\": 0 for key in neg_patterns})\n",
    "    \n",
    "    # Check if sdh falls under history_patterns\n",
    "    sdh_history_matches = [(m.start(), m.end()) for m in re.finditer(history_patterns[\"sdh\"], text)]\n",
    "    if sdh_history_matches:\n",
    "        matches[\"history_sdh\"] = 1\n",
    "    else:\n",
    "        matches[\"history_sdh\"] = 0\n",
    "\n",
    "    # Step 1: Check for negative patterns first\n",
    "    for key in neg_patterns:\n",
    "        neg_matches = [(m.start(), m.end()) for m in re.finditer(neg_patterns[key], text)]\n",
    "        if neg_matches:\n",
    "            matches[f\"{key}_neg\"] = 1\n",
    "\n",
    "    # Step 2: Check for positive patterns that are not within any negative pattern\n",
    "    for key_pos in pos_patterns:\n",
    "        pos_matches = [(m.start(), m.end()) for m in re.finditer(pos_patterns[key_pos], text)]\n",
    "        if pos_matches:\n",
    "            # Check if positive match is within history_patterns[\"sdh\"]\n",
    "            pos_within_sdh_history = False\n",
    "            for start, end in pos_matches:\n",
    "                for h_start, h_end in sdh_history_matches:\n",
    "                    if h_start <= start <= h_end or h_start <= end <= h_end:\n",
    "                        pos_within_sdh_history = True\n",
    "                        break\n",
    "                if pos_within_sdh_history:\n",
    "                    break\n",
    "            if not pos_within_sdh_history:\n",
    "                # Check if positive match is not within any negative pattern\n",
    "                is_positive = True\n",
    "                for start, end in pos_matches:\n",
    "                    for key_neg in neg_patterns:\n",
    "                        neg_matches = [(m.start(), m.end()) for m in re.finditer(neg_patterns[key_neg], text)]\n",
    "                        for n_start, n_end in neg_matches:\n",
    "                            if n_start <= start <= n_end or n_start <= end <= n_end:\n",
    "                                is_positive = False\n",
    "                                break\n",
    "                        if not is_positive:\n",
    "                            break\n",
    "                    if is_positive:\n",
    "                        matches[f\"{key_pos}_pos\"] = 1\n",
    "                        break\n",
    "\n",
    "    return matches\n",
    "\n",
    "\n",
    "# Applying all of the functions above\n",
    "feature_matrix = df['NoteContent'].apply(lambda note_text: pd.Series(check_patterns(note_text, neg_patterns, pos_patterns, history_patterns)))\n",
    "result_df = pd.concat([df[['BDSPPatientID', 'ContactDate', 'NoteFileName', 'Site', 'CT', 'MRI']], feature_matrix], axis=1)\n",
    "\n",
    "# Save result to CSV file\n",
    "result_df.to_csv('feature_matrix_MGB_.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/gregory178/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "## Here we are creating the feature matrix using key words for BIDMC\n",
    "\n",
    "# Importing necessary modules\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Initialize the Snowball Stemmer for English\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "# Define keywords and phrases\n",
    "keywords = [\n",
    "    \"acut sdh\", \"brain injuri\", \"brain mri\",\n",
    "    \"burr hole\", \"stabl sdh\", \"craniectomi\", \"craniotomi\", \"ct head\", \"drainag\",\n",
    "    \"evacu\", \"head ct\", \"hematoma\",\"chronic sdh\",\n",
    "    \"herniat\", \"intracrani hemorrhag\", \"intraparenchym hemorrhag\",\n",
    "    \"midlin shift\", \"mva\", \"mvc\", \"neurosurgeri\", \"neurosurg intervent\",\n",
    "    \"scan\", \"sdh\", \"subdur\", \"tbi\", \"tentorium\", \"thick\", \"trauma\", \"prior sdh\", \"recent sdh\", \"resolv sdh\",\n",
    "    \"known sdh\"\n",
    "]\n",
    "\n",
    "# Function to preprocess text (lowercase, stemming using NLTK)\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  # Convert text to lowercase\n",
    "    # Replace specific terms as needed\n",
    "    text = re.sub(r\"\\bsubdural (hematoma|hemorrhage)\\b\", \"sdh\", text)  # Example replacement\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "    return ' '.join(stemmed_tokens)\n",
    "\n",
    "\n",
    "# Preprocess the 'NoteContent' column\n",
    "df1['NoteContent'] = df1['NoteContent'].apply(preprocess_text)\n",
    "\n",
    "#history pattern. Only want to apply to sdh. \n",
    "history_patterns = {}\n",
    "sdh_pattern = r\"\\b(?:histori\\b|histori:\\b|hx\\b|pth\\b|pmh\\b)[^,.]{0,20}\\bsd(?:h)?\\b\"\n",
    "history_patterns[\"sdh\"] = sdh_pattern\n",
    "\n",
    "\n",
    "# generate neg and pos patterns\n",
    "pos_patterns = {}\n",
    "for phrase in keywords:\n",
    "    #stemmed_phrase = stem_phrase(phrase, stemmer)\n",
    "    words = phrase.split()\n",
    "    pos_patterns[phrase] = r\"\\b\" + r\"\".join([f\"{word}\\s*\" for word in words]) +r\"\\b\"\n",
    "\n",
    "# Adding negator words\n",
    "neg_patterns = {}\n",
    "for phrase in keywords:\n",
    "    #stemmed_phrase = stem_phrase(phrase, stemmer)  # no need since the keywords are already in basic form\n",
    "    words = phrase.split()\n",
    "    neg_patterns[phrase] = r\"\\b\" + r\"(?:(?:negat|no\\b|not\\b)[^,.]{{0,20}}{pattern})|(?:{pattern}[^.]{{0,10}}(?:absent\\b|negat))\".format(\n",
    "        pattern= r''.join([f\"{word}\\s*\" for word in words]) +r\"\\b\" )\n",
    "\n",
    "\n",
    "\n",
    "def check_patterns(text, neg_patterns, pos_patterns, history_patterns):\n",
    "    matches = {f\"{key}_pos\": 0 for key in pos_patterns}\n",
    "    matches.update({f\"{key}_neg\": 0 for key in neg_patterns})\n",
    "    \n",
    "    # Check if sdh falls under history_patterns\n",
    "    sdh_history_matches = [(m.start(), m.end()) for m in re.finditer(history_patterns[\"sdh\"], text)]\n",
    "    if sdh_history_matches:\n",
    "        matches[\"history_sdh\"] = 1\n",
    "    else:\n",
    "        matches[\"history_sdh\"] = 0\n",
    "\n",
    "    # Step 1: Check for negative patterns first\n",
    "    for key in neg_patterns:\n",
    "        neg_matches = [(m.start(), m.end()) for m in re.finditer(neg_patterns[key], text)]\n",
    "        if neg_matches:\n",
    "            matches[f\"{key}_neg\"] = 1\n",
    "\n",
    "    # Step 2: Check for positive patterns that are not within any negative pattern\n",
    "    for key_pos in pos_patterns:\n",
    "        pos_matches = [(m.start(), m.end()) for m in re.finditer(pos_patterns[key_pos], text)]\n",
    "        if pos_matches:\n",
    "            # Check if positive match is within history_patterns[\"sdh\"]\n",
    "            pos_within_sdh_history = False\n",
    "            for start, end in pos_matches:\n",
    "                for h_start, h_end in sdh_history_matches:\n",
    "                    if h_start <= start <= h_end or h_start <= end <= h_end:\n",
    "                        pos_within_sdh_history = True\n",
    "                        break\n",
    "                if pos_within_sdh_history:\n",
    "                    break\n",
    "            if not pos_within_sdh_history:\n",
    "                # Check if positive match is not within any negative pattern\n",
    "                is_positive = True\n",
    "                for start, end in pos_matches:\n",
    "                    for key_neg in neg_patterns:\n",
    "                        neg_matches = [(m.start(), m.end()) for m in re.finditer(neg_patterns[key_neg], text)]\n",
    "                        for n_start, n_end in neg_matches:\n",
    "                            if n_start <= start <= n_end or n_start <= end <= n_end:\n",
    "                                is_positive = False\n",
    "                                break\n",
    "                        if not is_positive:\n",
    "                            break\n",
    "                    if is_positive:\n",
    "                        matches[f\"{key_pos}_pos\"] = 1\n",
    "                        break\n",
    "\n",
    "    return matches\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Applying functions above to make the feature matrix\n",
    "feature_matrix = df1['NoteContent'].apply(lambda note_text: pd.Series(check_patterns(note_text, neg_patterns, pos_patterns, history_patterns)))\n",
    "result_df1 = pd.concat([df1[['BDSPPatientID', 'ContactDate', 'NoteFileName', 'Site', 'CT', 'MRI']], feature_matrix], axis=1)\n",
    "#Need to add MRI and CT here \n",
    "\n",
    "# Save result to CSV file\n",
    "result_df1.to_csv('feature_matrix_BIDMC_.csv', index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['BDSPPatientID', 'ContactDate', 'NoteFileName', 'Site', 'CT', 'MRI',\n",
      "       'acut sdh_pos', 'brain injuri_pos', 'brain mri_pos', 'burr hole_pos',\n",
      "       'stabl sdh_pos', 'craniectomi_pos', 'craniotomi_pos', 'ct head_pos',\n",
      "       'drainag_pos', 'evacu_pos', 'head ct_pos', 'hematoma_pos',\n",
      "       'chronic sdh_pos', 'herniat_pos', 'intracrani hemorrhag_pos',\n",
      "       'intraparenchym hemorrhag_pos', 'midlin shift_pos', 'mva_pos',\n",
      "       'mvc_pos', 'neurosurgeri_pos', 'neurosurg intervent_pos', 'scan_pos',\n",
      "       'sdh_pos', 'subdur_pos', 'tbi_pos', 'tentorium_pos', 'thick_pos',\n",
      "       'trauma_pos', 'prior sdh_pos', 'recent sdh_pos', 'resolv sdh_pos',\n",
      "       'known sdh_pos', 'acut sdh_neg', 'brain injuri_neg', 'brain mri_neg',\n",
      "       'burr hole_neg', 'stabl sdh_neg', 'craniectomi_neg', 'craniotomi_neg',\n",
      "       'ct head_neg', 'drainag_neg', 'evacu_neg', 'head ct_neg',\n",
      "       'hematoma_neg', 'chronic sdh_neg', 'herniat_neg',\n",
      "       'intracrani hemorrhag_neg', 'intraparenchym hemorrhag_neg',\n",
      "       'midlin shift_neg', 'mva_neg', 'mvc_neg', 'neurosurgeri_neg',\n",
      "       'neurosurg intervent_neg', 'scan_neg', 'sdh_neg', 'subdur_neg',\n",
      "       'tbi_neg', 'tentorium_neg', 'thick_neg', 'trauma_neg', 'prior sdh_neg',\n",
      "       'recent sdh_neg', 'resolv sdh_neg', 'known sdh_neg', 'history_sdh'],\n",
      "      dtype='object')\n",
      "Index(['BDSPPatientID', 'ContactDate', 'NoteFileName', 'Site', 'CT', 'MRI',\n",
      "       'acut sdh_pos', 'brain injuri_pos', 'brain mri_pos', 'burr hole_pos',\n",
      "       'stabl sdh_pos', 'craniectomi_pos', 'craniotomi_pos', 'ct head_pos',\n",
      "       'drainag_pos', 'evacu_pos', 'head ct_pos', 'hematoma_pos',\n",
      "       'chronic sdh_pos', 'herniat_pos', 'intracrani hemorrhag_pos',\n",
      "       'intraparenchym hemorrhag_pos', 'midlin shift_pos', 'mva_pos',\n",
      "       'mvc_pos', 'neurosurgeri_pos', 'neurosurg intervent_pos', 'scan_pos',\n",
      "       'sdh_pos', 'subdur_pos', 'tbi_pos', 'tentorium_pos', 'thick_pos',\n",
      "       'trauma_pos', 'prior sdh_pos', 'recent sdh_pos', 'resolv sdh_pos',\n",
      "       'known sdh_pos', 'acut sdh_neg', 'brain injuri_neg', 'brain mri_neg',\n",
      "       'burr hole_neg', 'stabl sdh_neg', 'craniectomi_neg', 'craniotomi_neg',\n",
      "       'ct head_neg', 'drainag_neg', 'evacu_neg', 'head ct_neg',\n",
      "       'hematoma_neg', 'chronic sdh_neg', 'herniat_neg',\n",
      "       'intracrani hemorrhag_neg', 'intraparenchym hemorrhag_neg',\n",
      "       'midlin shift_neg', 'mva_neg', 'mvc_neg', 'neurosurgeri_neg',\n",
      "       'neurosurg intervent_neg', 'scan_neg', 'sdh_neg', 'subdur_neg',\n",
      "       'tbi_neg', 'tentorium_neg', 'thick_neg', 'trauma_neg', 'prior sdh_neg',\n",
      "       'recent sdh_neg', 'resolv sdh_neg', 'known sdh_neg', 'history_sdh'],\n",
      "      dtype='object')\n",
      "[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True]\n"
     ]
    }
   ],
   "source": [
    "#Print columns. \n",
    "print(result_df.columns)\n",
    "print(result_df1.columns)\n",
    "print(result_df.columns==result_df1.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BDSPPatientID</th>\n",
       "      <th>ContactDate</th>\n",
       "      <th>NoteFileName</th>\n",
       "      <th>Site</th>\n",
       "      <th>CT</th>\n",
       "      <th>MRI</th>\n",
       "      <th>acut sdh_pos</th>\n",
       "      <th>brain injuri_pos</th>\n",
       "      <th>brain mri_pos</th>\n",
       "      <th>burr hole_pos</th>\n",
       "      <th>...</th>\n",
       "      <th>subdur_neg</th>\n",
       "      <th>tbi_neg</th>\n",
       "      <th>tentorium_neg</th>\n",
       "      <th>thick_neg</th>\n",
       "      <th>trauma_neg</th>\n",
       "      <th>prior sdh_neg</th>\n",
       "      <th>recent sdh_neg</th>\n",
       "      <th>resolv sdh_neg</th>\n",
       "      <th>known sdh_neg</th>\n",
       "      <th>history_sdh</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>120109726</td>\n",
       "      <td>2019-12-01</td>\n",
       "      <td>Notes_13393227243_2508134861_20191201.txt</td>\n",
       "      <td>MGB</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>111971091</td>\n",
       "      <td>2021-05-30</td>\n",
       "      <td>Notes_13517098931_6064385669_20210530.txt</td>\n",
       "      <td>MGB</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>114651683</td>\n",
       "      <td>2022-03-09</td>\n",
       "      <td>Notes_13621620103_9363901699_20220309.txt</td>\n",
       "      <td>MGB</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>115288640</td>\n",
       "      <td>2021-03-02</td>\n",
       "      <td>Notes_13554067774_6841967671_20210302.txt</td>\n",
       "      <td>MGB</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>115389340</td>\n",
       "      <td>2019-12-02</td>\n",
       "      <td>Notes_13481394171_4791230696_20191202.txt</td>\n",
       "      <td>MGB</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 71 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   BDSPPatientID ContactDate                               NoteFileName Site  \\\n",
       "0      120109726  2019-12-01  Notes_13393227243_2508134861_20191201.txt  MGB   \n",
       "1      111971091  2021-05-30  Notes_13517098931_6064385669_20210530.txt  MGB   \n",
       "2      114651683  2022-03-09  Notes_13621620103_9363901699_20220309.txt  MGB   \n",
       "3      115288640  2021-03-02  Notes_13554067774_6841967671_20210302.txt  MGB   \n",
       "4      115389340  2019-12-02  Notes_13481394171_4791230696_20191202.txt  MGB   \n",
       "\n",
       "   CT  MRI  acut sdh_pos  brain injuri_pos  brain mri_pos  burr hole_pos  ...  \\\n",
       "0   0    1             0                 0              0              0  ...   \n",
       "1   0    1             0                 0              0              0  ...   \n",
       "2   0    1             0                 1              0              0  ...   \n",
       "3   1    1             0                 0              0              0  ...   \n",
       "4   1    1             0                 0              0              0  ...   \n",
       "\n",
       "   subdur_neg  tbi_neg  tentorium_neg  thick_neg  trauma_neg  prior sdh_neg  \\\n",
       "0           0        0              0          0           0              0   \n",
       "1           0        0              0          0           0              0   \n",
       "2           0        0              0          0           0              0   \n",
       "3           0        0              0          0           0              0   \n",
       "4           0        0              0          0           0              0   \n",
       "\n",
       "   recent sdh_neg  resolv sdh_neg  known sdh_neg  history_sdh  \n",
       "0               0               0              0            0  \n",
       "1               0               0              0            0  \n",
       "2               0               0              0            0  \n",
       "3               0               0              0            0  \n",
       "4               0               0              0            0  \n",
       "\n",
       "[5 rows x 71 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine the MGB and BIDMC feature matrixes with CPT codes. \n",
    "MGB_BIDMC_CPT_Feature_Matrix= pd.concat([result_df,result_df1], axis=0, ignore_index=True)\n",
    "MGB_BIDMC_CPT_Feature_Matrix.to_csv(\"MGB_BIDMC_CPT_Feature_Matrix_.csv\", index=False)\n",
    "MGB_BIDMC_CPT_Feature_Matrix.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
